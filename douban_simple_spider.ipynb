{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 豆瓣电影爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import time \n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from retrying import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本作品爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 豆瓣：基本作品爬取爬虫\n",
    "### 使用requests + BeautifulSoup\n",
    "### 请求头使用fake_useragent\n",
    "### 入口是豆瓣tag页面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基础爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "donghua_tag = \"https://movie.douban.com/tag/%E5%8A%A8%E7%94%BB\"\n",
    "def proxy_abuyun():\n",
    "    # 代理服务器\n",
    "    proxyHost = \"http-dyn.abuyun.com\"\n",
    "    proxyPort = \"9020\"\n",
    "\n",
    "    # 代理隧道验证信息\n",
    "    proxyUser = \"12345678\" # 需要自己改一下\n",
    "    proxyPass = \"12345678\" # 需要自己改一下\n",
    "\n",
    "    # 代理页面构成\n",
    "    proxyMeta = \"http://%(user)s:%(pass)s@%(host)s:%(port)s\" % {\n",
    "        \"host\" : proxyHost,\n",
    "        \"port\" : proxyPort,\n",
    "        \"user\" : proxyUser,\n",
    "        \"pass\" : proxyPass,\n",
    "    }\n",
    "    # 代理地址构造\n",
    "    proxies = {\n",
    "        \"http\"  : proxyMeta,\n",
    "        \"https\" : proxyMeta,\n",
    "    }\n",
    "    return proxies\n",
    "\n",
    "def requests_basic(url,maxpage):\n",
    "    bash = 20\n",
    "    url = requests.get(url)\n",
    "    list_all_divs = []\n",
    "    for i in range(1,maxpage):\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\":ua.random}\n",
    "        start_num = i * bash\n",
    "        complete_url = donghua_tag + \"?start=\" + str(start_num) +  \"&type=T\"\n",
    "        print(complete_url)\n",
    "        html = requests.get(complete_url,headers = headers)\n",
    "        html_soup = BeautifulSoup(html.text,'lxml')\n",
    "        all_divs = html_soup.find(\"div\",class_ = \"article\").find_all(\"table\",{\"width\":\"100%\"})\n",
    "        list_all_divs.append(all_divs)\n",
    "    return list_all_divs\n",
    "\n",
    "def parse(list_all_divs,maxpage):\n",
    "    list_hrefs,list_titles = [],[]\n",
    "    for page in range(0,maxpage-1):\n",
    "        print(page)\n",
    "        for item in list_all_divs[page]:\n",
    "            try:\n",
    "                info = item.find_all(\"a\",class_ = \"nbg\")\n",
    "                href = info[0].get(\"href\")\n",
    "                title = info[0].get(\"title\")\n",
    "                list_hrefs.append(href)\n",
    "                list_titles.append(title)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    table_first_layer = pd.DataFrame(list_hrefs,list_titles)\n",
    "    return table_first_layer\n",
    "\n",
    "#list_all_divs = requests_basic(donghua_tag,5)\n",
    "#table_first_layer = parse(list_all_divs,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同步爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同步爬虫练习\n",
    "# coding:utf-8\n",
    "# https://blog.csdn.net/feiyang123_/article/details/44700219?utm_source=blogxgwz5\n",
    "\n",
    "import time\n",
    "from lxml import etree\n",
    "import urllib.request as request\n",
    "\n",
    "# 目标网址\n",
    "url = \"https://movie.douban.com/tag/%E6%97%A5%E6%9C%AC?start=20&type=T\"\n",
    "def get_all_links(url:str)->list:\n",
    "    # 获取所有的链接\n",
    "    response = request.urlopen(url)\n",
    "    wb_data = response.read()\n",
    "    html = etree.HTML(wb_data)\n",
    "    first_layer_links = html.xpath('//a[@class=\"nbg\"]/@href')\n",
    "    return first_layer_links\n",
    "\n",
    "def get_all_titles(url:str)->list:\n",
    "    # 获取所有的标题\n",
    "    response = request.urlopen(url)\n",
    "    wb_data = response.read()\n",
    "    html = etree.HTML(wb_data)\n",
    "    first_layer_titles = html.xpath('//a[@class=\"nbg\"]/@title')\n",
    "    return first_layer_titles\n",
    "\n",
    "def generage_first_layer_table(url):\n",
    "    # 获取表格\n",
    "    start = time.time()\n",
    "    urls = get_all_links(url)\n",
    "    titles = get_all_titles(url)\n",
    "    table_first_layer = pd.DataFrame(urls,titles)\n",
    "    end = time.time()\n",
    "    print(u\"下载完成,耗时:%.2fs\" % (end - start))\n",
    "    return table_first_layer\n",
    "\n",
    "def allot(first_layer_links:list,n:int)->list:\n",
    "    # 根据给定的组数，分配url给每一组\n",
    "    _len = len(first_layer_links)\n",
    "    base = int(_len / n)\n",
    "    remainer = _len % n\n",
    "    groups = [first_layer_links[i * base:(i + 1) * base] for i in range(n)]\n",
    "    remaind_group = first_layer_links[n * base:]\n",
    "    for i in range(remainer):\n",
    "        groups[i].append(remaind_group[i])\n",
    "    return [i for i in groups if i]\n",
    "\n",
    "def crawler():\n",
    "    generage_first_layer_table(url)\n",
    "    \n",
    "#crawler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多进程爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多进程爬虫\n",
    "from multiprocessing.pool import Pool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def request_single_page(url):\n",
    "    # 请求单个链接\n",
    "    ua = UserAgent()\n",
    "    headers = {\"User-Agent\":ua.random}\n",
    "    response = requests.get(url,headers = headers)\n",
    "    html = etree.HTML(response.text)\n",
    "    title = html.xpath('//span[@property=\"v:itemreviewed\"]/text()')\n",
    "    print(title)\n",
    "    return title\n",
    "\n",
    "def request_multiple_pages(urls):\n",
    "    # 请求多个链接\n",
    "    start = time.time()\n",
    "    index = 0\n",
    "    for url in urls:\n",
    "        index += 1\n",
    "        print(index)\n",
    "        title = request_single_page(url)\n",
    "        print(title)\n",
    "    end = time.time()\n",
    "    print(u'下载完成,耗时:%.2fs' % (end - start))\n",
    "\n",
    "def multiprocess_crawler(processors:int,first_layer_links:list):\n",
    "    # 多进程爬虫\n",
    "    pool = Pool(processors)\n",
    "    # 对应多进程的进程数processors,将待爬链接分组\n",
    "    url_groups = allot(first_layer_links,processors)\n",
    "    print(url_groups)\n",
    "    for i in url_groups:\n",
    "        pool.apply_async(func = request_multiple_pages,args = (i,))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# first_layer_links = get_all_links(url)\n",
    "# request_multiple_pages(first_layer_links)\n",
    "# multiprocess_crawler(2,first_layer_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多线程爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 目前这个可用\n",
    "\n",
    "import random\n",
    "from threading import Thread\n",
    "\n",
    "def run_multithread_crawler(first_layer_links:list,threads:int):\n",
    "    begin = 0\n",
    "    start = time.time()\n",
    "    while 1:\n",
    "        _threads = []\n",
    "        urls = first_layer_links[begin:begin+threads]\n",
    "        if not urls:\n",
    "            break\n",
    "        for i in urls:\n",
    "            t = Thread(target = request_single_page,args = (i,))\n",
    "            _threads.append(t)\n",
    "        for t in _threads:\n",
    "            t.setDaemon(True)\n",
    "            t.start()\n",
    "        for t in _threads:\n",
    "            t.join()\n",
    "        begin += threads\n",
    "    end = time.time()\n",
    "    print(u'下载完成,耗时:%.2fs' % ((end - start)))\n",
    "    return _threads\n",
    "    \n",
    "def multithreads_crawler(threads:int):\n",
    "    first_layer_links = get_all_links(url)\n",
    "    run_multithread_crawler(first_layer_links,threads)\n",
    "    \n",
    "#_threads = multithreads_crawler(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异步协程爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 目前这个也可用\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "from aiohttp import ClientSession,TCPConnector\n",
    "\n",
    "async def download(session:ClientSession,url:str,sem:Semaphore):\n",
    "    ## 请求模块\n",
    "    async with sem:\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\":ua.random}\n",
    "        async with session.get(url,headers = headers) as response:\n",
    "            result = await response.read()\n",
    "            html_soup = BeautifulSoup(result,'lxml')\n",
    "            print(html_soup.find(\"h1\"))\n",
    "            \n",
    "async def run_coroutine_crawler(first_layer_links:list,concurrency:int):\n",
    "    # 异步协程爬虫，最大并发请求数concurrency\n",
    "    tasks = []\n",
    "    sem = Semaphore(concurrency)\n",
    "    conn = TCPConnector(limit = concurrency)\n",
    "    async with ClientSession(connector = conn) as session:\n",
    "        for i in first_layer_links:\n",
    "            tasks.append(asyncio.create_task(download(session,i,sem)))\n",
    "        start = time.time()\n",
    "        await asyncio.gather(*tasks)\n",
    "        end = time.time()\n",
    "        print(u'下载完成,耗时:%.2fs' % ((end - start)))\n",
    "        \n",
    "def coroutine_crawler(concurrency:int,url:str):\n",
    "    first_layer_links = get_all_links(url)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(run_coroutine_crawler(first_layer_links,concurrency))\n",
    "    loop.close()\n",
    "\n",
    "def execute():\n",
    "    url = \"https://movie.douban.com/tag/%E6%97%A5%E6%9C%AC?start=20&type=T\"\n",
    "    coroutine_crawler(100,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多进程 + 多线程爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_process_thread_crawler(first_layers_links:list,processors:int,threads:int):\n",
    "    pool = Pool(processors)\n",
    "    first_layer_links = get_all_links(url)\n",
    "    url_groups = allot(first_layers_links,processors)\n",
    "    for group in url_groups:\n",
    "        pool.apply_async(run_multithread_crawler,args = (group,threads))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "#first_layer_links = get_all_links(url)\n",
    "#mixed_process_thread_crawler(first_layer_links,4,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多进程 + 异步协程爬虫练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coroutine_crawler(first_layers_links:list,concurrency:int):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(run_coroutine_crawler(first_layers_links,concurrency))\n",
    "    loop.close()\n",
    "\n",
    "def mixed_process_coroutine_crawler(processors:int,concurrency:int):\n",
    "    pool = Pool(processors)\n",
    "    first_layer_links = get_all_links(url)\n",
    "    url_groups = allot(pic_urls, processors)\n",
    "    for group in url_groups:\n",
    "        pool.apply_async(_coroutine_crawler, args=(group, concurrency))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 协程原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! (<_MainThread(MainThread, started 24448)>)\n",
      "Hello world! (<_MainThread(MainThread, started 24448)>)\n",
      "Hello again! (<_MainThread(MainThread, started 24448)>)\n",
      "Hello again! (<_MainThread(MainThread, started 24448)>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({<Task finished coro=<hello() done, defined at <ipython-input-10-58e59cd72d29>:4> result=None>,\n",
       "  <Task finished coro=<hello() done, defined at <ipython-input-10-58e59cd72d29>:4> result=None>},\n",
       " set())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "import asyncio\n",
    "\n",
    "async def hello():\n",
    "    print('Hello world! (%s)' % threading.currentThread())\n",
    "    await asyncio.sleep(5)\n",
    "    print('Hello again! (%s)' % threading.currentThread())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [hello(),hello()]\n",
    "loop.run_until_complete(asyncio.wait(tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aiohttp构建异步爬虫框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aiohttp:requests案例\n",
    "# https://zhuanlan.zhihu.com/p/36936574\n",
    "# https://hubertroy.gitbooks.io/aiohttp-chinese-documentation/content/\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(session,url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "    \n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session,\"http://www.baidu.com\") \n",
    "        #print(html)\n",
    "        \n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共耗时0.8790502548217773s\n",
      "{URL('https://blog.csdn.net/jclian91/article/details/82691269'): '利用aiohttp实现异步爬虫 - 但盼风雨来 - CSDN博客'}\n"
     ]
    }
   ],
   "source": [
    "# http://edmundmartin.com/concurrent-crawling-in-python/\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "class AsnycGrab(object):\n",
    "    def __init__(self, url_list, max_threads):\n",
    "        self.urls = url_list\n",
    "        ### 设置results词典\n",
    "        self.results = {}\n",
    "        ### 设置最大线程数量\n",
    "        self.max_threads = max_threads\n",
    "\n",
    "    def _parse_results(self,url,html):\n",
    "        \"\"\"\n",
    "        解析部分\n",
    "        param url:url_list中的任一页面\n",
    "        param html:url通过get_body后请求得到的结果\n",
    "        return results中以字典的形式，存档title\n",
    "        \"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "            title = soup.find(\"title\").get_text()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        if title:\n",
    "            self.results[url] = title\n",
    "            \n",
    "    async def get_body(self,url):\n",
    "        \"\"\"\n",
    "        异步请求部分\n",
    "        param url:url_list中的任一页面\n",
    "        return html:请求后得到的status\n",
    "        \"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url,timeout = 30) as response:\n",
    "                assert response.status == 200\n",
    "                html = await response.read()\n",
    "                return response.url,html\n",
    "            \n",
    "    async def get_results(self,url):\n",
    "        \"\"\"\n",
    "        执行请求和解析两块\n",
    "        \"\"\"\n",
    "        url,html = await self.get_body(url)\n",
    "        self._parse_results(url,html)\n",
    "        return 'Completed'\n",
    "    \n",
    "    async def handle_tasks(self, task_id, work_queue):\n",
    "        ### 使用线程来分配任务！\n",
    "        while not work_queue.empty():\n",
    "            current_url = await work_queue.get()\n",
    "            try:\n",
    "                task_status = await self.get_results(current_url)\n",
    "            except Exception as e:\n",
    "                logging.exception('Error for {}'.format(current_url), exc_info=True)\n",
    "                \n",
    "    def eventloop(self):\n",
    "        ### 设置任务队列！\n",
    "        q = asyncio.Queue()\n",
    "        [q.put_nowait(url) for url in self.urls]\n",
    "        loop = asyncio.get_event_loop()\n",
    "        tasks = [self.handle_tasks(task_id, q, ) for task_id in range(self.max_threads)]\n",
    "        start = time.time()\n",
    "        loop.run_until_complete(asyncio.wait(tasks))\n",
    "        end = time.time()\n",
    "        print(\"一共耗时\" + str(end - start) + \"s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_urls = [\"https://blog.csdn.net/jclian91/article/details/82691269\"] * 5\n",
    "    async_example = AsnycGrab(test_urls, 5)\n",
    "    async_example.eventloop()\n",
    "    print(async_example.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共耗时2.2301275730133057s\n"
     ]
    }
   ],
   "source": [
    "### 和最简单爬虫的对比\n",
    "def simplest_parse(url):\n",
    "    html = requests.get(url)\n",
    "    html_soup = BeautifulSoup(html.text,'lxml')\n",
    "    title = html_soup.find(\"title\").get_text()\n",
    "    \n",
    "test_urls = [\"https://blog.csdn.net/jclian91/article/details/82691269\"] * 5 \n",
    "start = time.time()\n",
    "for url in test_urls:\n",
    "    simplest_parse(url)\n",
    "end = time.time()\n",
    "print(\"一共耗时\" + str(end - start) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本页面信息爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 豆瓣：作品基本页面信息爬虫\n",
    "### 使用requests + BeautifulSoup\n",
    "### 请求头使用fake_useragent\n",
    "### 单线程&携程\n",
    "\n",
    "list_soup = []\n",
    "table = pd.read_excel(\"table_check_3.xlsx\")\n",
    "\n",
    "def proxy_abuyun():\n",
    "    # 代理服务器\n",
    "    proxyHost = \"http-dyn.abuyun.com\"\n",
    "    proxyPort = \"9020\"\n",
    "\n",
    "    # 代理隧道验证信息\n",
    "    proxyUser = \"H12AUU1SFVJ07G9D\"\n",
    "    proxyPass = \"5F45BADF5392BD43\"\n",
    "\n",
    "    # 代理页面构成\n",
    "    proxyMeta = \"http://%(user)s:%(pass)s@%(host)s:%(port)s\" % {\n",
    "        \"host\" : proxyHost,\n",
    "        \"port\" : proxyPort,\n",
    "        \"user\" : proxyUser,\n",
    "        \"pass\" : proxyPass,\n",
    "    }\n",
    "    # 代理\n",
    "    proxies = {\n",
    "        \"http\"  : proxyMeta,\n",
    "        \"https\" : proxyMeta,\n",
    "    }\n",
    "    return proxies\n",
    "\n",
    "@retry(stop_max_attempt_number=3)\n",
    "def requests_single_thread(table,proxies):\n",
    "    index = 0\n",
    "    for item in table[\"url\"]:\n",
    "        try:\n",
    "            ua = UserAgent()\n",
    "            headers = {\"User-Agent\":ua.random}\n",
    "            html = requests.get(item,headers = headers,proxies = proxies,timeout = 30)\n",
    "            html_soup = BeautifulSoup(html.text,'lxml')\n",
    "            list_soup.append(html_soup)\n",
    "            index += 1 \n",
    "            if index % 10 == 0:\n",
    "                time.sleep(5)\n",
    "            if index % 50 == 0:\n",
    "                time.sleep(10 + random.randint(-5,5))\n",
    "            print(index)\n",
    "        except Exception as e:\n",
    "            print(\"error\")\n",
    "            list_soup.append(\"error\")\n",
    "    return list_soup\n",
    "        \n",
    "def requests_coroutines(table):\n",
    "    pass\n",
    "\n",
    "proxies = proxy_abuyun()\n",
    "requests_single_thread(table,proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_title,list_time,list_genre = [],[],[]\n",
    "\n",
    "def parse_to_table(list_soup,list_title,list_time,list_genre):\n",
    "    for html_soup in list_soup:\n",
    "        try:\n",
    "            title = html_soup.find(\"h1\").find(\"span\",{\"property\":\"v:itemreviewed\"}).get_text()\n",
    "            list_title.append(title)\n",
    "        except Exception as e:\n",
    "            list_title.append(\"None\")\n",
    "        try:\n",
    "            time = html_soup.find(\"span\",{\"property\":\"v:initialReleaseDate\"}).get_text()\n",
    "            list_time.append(time)\n",
    "        except Exception as e:\n",
    "            list_time.append(\"None\")\n",
    "        try:\n",
    "            genres = html_soup.find_all(\"span\",{\"property\":\"v:genre\"})\n",
    "            gen_li = []\n",
    "            if genres == []:\n",
    "                list_genre.append(\"None\")\n",
    "            else:\n",
    "                for g in genres:\n",
    "                    result = g.get_text()\n",
    "                    gen_li.append(result)\n",
    "                list_genre.append(\"，\".join(gen_li))\n",
    "                gen_li = []\n",
    "        except Exception as e:\n",
    "            list_genre.append(\"None\")\n",
    "    return list_title,list_time,list_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 页面短评 + 页面评论爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 待完工中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
